---
title: "InformationValue"
subtitle: "Performance Analysis and Companion Functions that aid Accuracy Improvement for Binary Classification Models"
author: "Selva Prabhakaran"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_caption: yes
    toc: true
    css: custom.css
vignette: >
  %\VignetteIndexEntry{Vignette Title Subtitle}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
The functions in `InformationValue` package are broadly divided in following categories:

1. Diagnostics of prediction probability scores
    + `plotROC`
    + `sensitivity`
    + `specificity`
    + `youdensIndex`
2. Performance analysis
    + `misClassError`
    + `kappaCohen`
    + `Concordance`
    + `somersD`
3. Functions that aid accuracy improvement
    + `optimalCutoff`
    + `WOE`
    + `WOETable`
    + `IV`

## 1. Diagnostics of prediction probability scores
### 1.1 `plotROC`
The plotROC uses the `ggplot2` framework to create the ROC curve and prints the `AUROC` inside. It comes with an option to display the change points of the prediction probability scores on the graph if you set the `Show.labels = T`.

```{r, echo=FALSE, results='asis', fig.show='hold', message=FALSE, warning=FALSE}
library(InformationValue)
```

```{r, results='asis', fig.show='hold', fig.height=5, fig.width=5, fig.cap = "ROC Curve", prompt=TRUE, highlight=TRUE, collapse=TRUE, message=FALSE, warning=FALSE}
data('ActualsAndScores')
plotROC(actuals=ActualsAndScores$Actuals, predictedScores=ActualsAndScores$PredictedScores)
```

You can also get the sensitivity matrix used to make the plot by turning on `returnSensitivityMat = TRUE`.

```{r, echo=TRUE, results='hide', fig.show='hide', prompt=TRUE, highlight=TRUE, tidy=TRUE, collapse=TRUE}
sensMat <- plotROC(actuals=ActualsAndScores$Actuals,  predictedScores=ActualsAndScores$PredictedScores, returnSensitivityMat = TRUE)
```
    Here is the first 10 rows of the resulting sensitivity matrix. The 'TPR' and 'FPR' are obtained for probability cutoff scores starting from 1 to 0 in steps of -0.02. 
```{r, echo=FALSE, results='asis', tab.cap="Top 10 rows of sensitivity matrix used to make the plot", prompt=TRUE, highlight=TRUE}
knitr::kable(head(sensMat, 10))
```  

### 1.2. `sensitivity`
Sensitivity, also considered as the 'True Positive Rate' is the proportion of 'Events' (or 'Ones') correctly predicted by the model, for a given prediction probability cutoff score. The default cutoff score for the `specificity` function unless specified by the `threshold` argument is taken as `0.5`.
```{r, results='asis', prompt=TRUE, highlight=TRUE, collapse=TRUE, message=FALSE, warning=FALSE, tidy=TRUE}
sensitivity(actuals = ActualsAndScores$Actuals, predictedScores = ActualsAndScores$PredictedScores)
```

If the objective of your problem is to maximise the ability of your model to detect the 'Events' (or 'Ones'), even at the cost of wrongly predicting the non-events ('Zeros') as an event ('One'), then you could set the threshold as determined by the `optimalCutoff()` with `optimiseFor='Ones'`   
```{r, results='asis', prompt=TRUE, highlight=TRUE, message=FALSE, warning=FALSE, tidy=TRUE}
max_sens_cutoff <- optimalCutoff(actuals=ActualsAndScores$Actuals, predictedScores = ActualsAndScores$PredictedScores, optimiseFor='Ones')  # determine cutoff to maximise sensitivity.

print(max_sens_cutoff)  # This would be cut-off score that achieved maximum sensitivity.

sensitivity(actuals = ActualsAndScores$Actuals, predictedScores = ActualsAndScores$PredictedScores, threshold=max_sens_cutoff)
```


### 1.3. `specificity`
For a given probability score cutoff (`threshold`), specificity computes what proportion of the total non-events (zeros) were predicted accurately. It can alo be computed as `1 - False Positive Rate`. If unless specified, the default `threshold` value is set as `0.5`, which means, the values of `ActualsAndScores$PredictedScores` above `0.5` is considered as events (Ones).
```{r, results='asis', prompt=TRUE, highlight=TRUE, collapse=FALSE, message=FALSE, warning=FALSE, tidy=TRUE}
specificity(actuals=ActualsAndScores$Actuals, predictedScores=ActualsAndScores$PredictedScores)
```

If you wish to know what proportion of non-events could be detected by lowering the `threshold`:
```{r, results='asis', prompt=TRUE, highlight=TRUE, collapse=FALSE, message=FALSE, warning=FALSE, tidy=TRUE}
specificity(actuals=ActualsAndScores$Actuals, predictedScores=ActualsAndScores$PredictedScores, threshold = 0.35)
```


### 1.4. `youdensIndex`
Youden’s J Index (Youden 1950), calculated as
$$J = Sensitivity + Specificity − 1$$
represents the proportions of correctly predicted observations for both the events (Ones) and nonevents (Zeros). IT is particularly useful if you want a single measure that accounts for both *false-positive* and *false-negative* rates

```{r, results='asis', prompt=TRUE, highlight=TRUE, collapse=TRUE, message=FALSE, warning=FALSE, tidy=TRUE, comment='#'}
youdensIndex(actuals=ActualsAndScores$Actuals, predictedScores=ActualsAndScores$PredictedScores)
```


## 2. Performance Analysis
### 2.1. `misClassError`
Mis-Classification Error is the proportion of all events that were incorrectly classified, for a given probability cutoff score.

```{r, results='asis', prompt=TRUE, highlight=TRUE, collapse=TRUE, message=FALSE, warning=FALSE, tidy=TRUE, comment='#'}
misClassError(actuals=ActualsAndScores$Actuals, predictedScores=ActualsAndScores$PredictedScores, threshold=0.5)
```


### 2.2. `kappaCohen`
[`Cohen's Kappa`](https://en.wikipedia.org/wiki/Cohen%27s_kappa) is a robust accuracy measure that accounts for the percentage aggreement that would occur by chance.

```{r, results='asis', prompt=TRUE, highlight=TRUE, collapse=TRUE, message=FALSE, warning=FALSE, tidy=TRUE, comment='#'}
kappaCohen(actuals=ActualsAndScores$Actuals, predictedScores=ActualsAndScores$PredictedScores)
```

### 2.3. `Concordance`
Concordance is the percentage of predicted probability scores where the scores of actual positive’s are greater than the scores of actual negative’s. It is calculated by taking into account the scores of all possible pairs of *Ones* and *Zeros*. 
    If the concordance of a model is 100%, it means that, by tweaking the prediction probability cutoff, we could accurately predict all of the events and non-events. 

```{r, results='asis', prompt=TRUE, highlight=TRUE, collapse=TRUE, message=FALSE, warning=FALSE, tidy=TRUE, comment='#'}
Concordance(actuals=ActualsAndScores$Actuals, predictedScores=ActualsAndScores$PredictedScores)
```

### 2.4. `somersD`
`somersD` computes how many more concordant than discordant pairs exist divided by the total number of pairs. Larger the Somers D value, better model's predictive ability.
$$Somers D = \frac{Concordant Pairs - Discordant Pairs}{Total Pairs} $$

```{r, results='asis', prompt=TRUE, highlight=TRUE, collapse=TRUE, message=FALSE, warning=FALSE, tidy=TRUE, comment='#'}
somersD(actuals=ActualsAndScores$Actuals, predictedScores=ActualsAndScores$PredictedScores)
```


## 3. Functions that aid accuracy improvement
### 3.1. `optimalCutoff`
### 3.2. `WOE`
### 3.3. `WOETable`
### 3.4. `IV`




> "He who gives up [code] safety for [code] speed deserves neither."
([via](https://twitter.com/hadleywickham/status/504368538874703872))
